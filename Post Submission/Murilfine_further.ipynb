{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import TrainingArguments, Trainer, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text).lower().strip()\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df = pd.read_csv('D:\\\\GDG_Hammad_ML\\\\dataset\\\\train.csv')\n",
    "df['comment'] = df['comment'].apply(clean_text)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"google/muril-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(df['label'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"fine_tuned_muril\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(df['label'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce2b44616ac04d04aa28a939a61efe08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/213747 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hf_dataset = Dataset.from_pandas(df)\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['comment'], padding=True, truncation=True, max_length=128)\n",
    "\n",
    "hf_dataset = hf_dataset.map(tokenize, batched=True)\n",
    "\n",
    "hf_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\doodlegen\\Lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d76a35148ad4fdfaf99b9c539520ddc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37406 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\doodlegen\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:435: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6561, 'grad_norm': 14.749953269958496, 'learning_rate': 2.667735899492115e-06, 'epoch': 0.03}\n",
      "{'loss': 0.6364, 'grad_norm': 9.855474472045898, 'learning_rate': 5.33547179898423e-06, 'epoch': 0.05}\n",
      "{'loss': 0.6547, 'grad_norm': 7.8750786781311035, 'learning_rate': 8.003207698476344e-06, 'epoch': 0.08}\n",
      "{'loss': 0.6129, 'grad_norm': 4.7021894454956055, 'learning_rate': 1.0676289762095697e-05, 'epoch': 0.11}\n",
      "{'loss': 0.586, 'grad_norm': 6.069854736328125, 'learning_rate': 1.334937182571505e-05, 'epoch': 0.13}\n",
      "{'loss': 0.5851, 'grad_norm': 8.404452323913574, 'learning_rate': 1.6017107725207165e-05, 'epoch': 0.16}\n",
      "{'loss': 0.5915, 'grad_norm': 11.278371810913086, 'learning_rate': 1.869018978882652e-05, 'epoch': 0.19}\n",
      "{'loss': 0.5651, 'grad_norm': 2.629645824432373, 'learning_rate': 1.98485073518491e-05, 'epoch': 0.21}\n",
      "{'loss': 0.5447, 'grad_norm': 8.691455841064453, 'learning_rate': 1.9551462943710086e-05, 'epoch': 0.24}\n",
      "{'loss': 0.5387, 'grad_norm': 15.004962921142578, 'learning_rate': 1.9254418535571068e-05, 'epoch': 0.27}\n",
      "{'loss': 0.5532, 'grad_norm': 7.424803256988525, 'learning_rate': 1.8957374127432053e-05, 'epoch': 0.29}\n",
      "{'loss': 0.5528, 'grad_norm': 15.407182693481445, 'learning_rate': 1.8660923808109316e-05, 'epoch': 0.32}\n",
      "{'loss': 0.5562, 'grad_norm': 27.16949462890625, 'learning_rate': 1.8364473488786576e-05, 'epoch': 0.35}\n",
      "{'loss': 0.5612, 'grad_norm': 5.418870449066162, 'learning_rate': 1.8067429080647557e-05, 'epoch': 0.37}\n",
      "{'loss': 0.5616, 'grad_norm': 1.6843434572219849, 'learning_rate': 1.7771572850141095e-05, 'epoch': 0.4}\n",
      "{'loss': 0.5683, 'grad_norm': 9.435197830200195, 'learning_rate': 1.747452844200208e-05, 'epoch': 0.43}\n",
      "{'loss': 0.5553, 'grad_norm': 16.72148323059082, 'learning_rate': 1.717748403386306e-05, 'epoch': 0.45}\n",
      "{'loss': 0.5579, 'grad_norm': 33.87483215332031, 'learning_rate': 1.6880439625724047e-05, 'epoch': 0.48}\n",
      "{'loss': 0.5928, 'grad_norm': 5.553280353546143, 'learning_rate': 1.658339521758503e-05, 'epoch': 0.51}\n",
      "{'loss': 0.5889, 'grad_norm': 19.85927963256836, 'learning_rate': 1.6286350809446013e-05, 'epoch': 0.53}\n",
      "{'loss': 0.5631, 'grad_norm': 8.707660675048828, 'learning_rate': 1.5989306401306998e-05, 'epoch': 0.56}\n",
      "{'loss': 0.5727, 'grad_norm': 2.182245969772339, 'learning_rate': 1.569226199316798e-05, 'epoch': 0.59}\n",
      "{'loss': 0.5809, 'grad_norm': 7.396358489990234, 'learning_rate': 1.5395217585028964e-05, 'epoch': 0.61}\n",
      "{'loss': 0.5853, 'grad_norm': 13.320059776306152, 'learning_rate': 1.5098173176889946e-05, 'epoch': 0.64}\n",
      "{'loss': 0.6014, 'grad_norm': 8.560729026794434, 'learning_rate': 1.480112876875093e-05, 'epoch': 0.67}\n",
      "{'loss': 0.5959, 'grad_norm': 6.5543107986450195, 'learning_rate': 1.4504084360611912e-05, 'epoch': 0.7}\n",
      "{'loss': 0.6177, 'grad_norm': 17.414016723632812, 'learning_rate': 1.4207039952472896e-05, 'epoch': 0.72}\n",
      "{'loss': 0.6144, 'grad_norm': 11.700026512145996, 'learning_rate': 1.3909995544333879e-05, 'epoch': 0.75}\n",
      "{'loss': 0.6095, 'grad_norm': 39.7306022644043, 'learning_rate': 1.3612951136194862e-05, 'epoch': 0.78}\n",
      "{'loss': 0.6005, 'grad_norm': 9.994001388549805, 'learning_rate': 1.3315906728055845e-05, 'epoch': 0.8}\n",
      "{'loss': 0.6149, 'grad_norm': 16.069002151489258, 'learning_rate': 1.301886231991683e-05, 'epoch': 0.83}\n",
      "{'loss': 0.6245, 'grad_norm': 11.865623474121094, 'learning_rate': 1.2722412000594088e-05, 'epoch': 0.86}\n",
      "{'loss': 0.622, 'grad_norm': 14.740219116210938, 'learning_rate': 1.2425367592455073e-05, 'epoch': 0.88}\n",
      "{'loss': 0.6221, 'grad_norm': 15.270381927490234, 'learning_rate': 1.2128323184316055e-05, 'epoch': 0.91}\n",
      "{'loss': 0.6686, 'grad_norm': 42.126461029052734, 'learning_rate': 1.183127877617704e-05, 'epoch': 0.94}\n",
      "{'loss': 0.6407, 'grad_norm': 9.421656608581543, 'learning_rate': 1.1534234368038021e-05, 'epoch': 0.96}\n",
      "{'loss': 0.6702, 'grad_norm': 15.847014427185059, 'learning_rate': 1.1237189959899006e-05, 'epoch': 0.99}\n",
      "{'loss': 0.6141, 'grad_norm': 10.378473281860352, 'learning_rate': 1.0940145551759988e-05, 'epoch': 1.02}\n",
      "{'loss': 0.6073, 'grad_norm': 18.064905166625977, 'learning_rate': 1.0643101143620973e-05, 'epoch': 1.04}\n",
      "{'loss': 0.5993, 'grad_norm': 12.81491756439209, 'learning_rate': 1.0346650824298234e-05, 'epoch': 1.07}\n",
      "{'loss': 0.5949, 'grad_norm': 26.565563201904297, 'learning_rate': 1.0049606416159216e-05, 'epoch': 1.1}\n",
      "{'loss': 0.603, 'grad_norm': 12.039793014526367, 'learning_rate': 9.7525620080202e-06, 'epoch': 1.12}\n",
      "{'loss': 0.5817, 'grad_norm': 19.843263626098633, 'learning_rate': 9.45611168869746e-06, 'epoch': 1.15}\n",
      "{'loss': 0.592, 'grad_norm': 14.718893051147461, 'learning_rate': 9.159067280558445e-06, 'epoch': 1.18}\n",
      "{'loss': 0.5707, 'grad_norm': 12.06700611114502, 'learning_rate': 8.862022872419429e-06, 'epoch': 1.2}\n",
      "{'loss': 0.5997, 'grad_norm': 16.40547752380371, 'learning_rate': 8.56497846428041e-06, 'epoch': 1.23}\n",
      "{'loss': 0.5667, 'grad_norm': 15.991778373718262, 'learning_rate': 8.268528144957672e-06, 'epoch': 1.26}\n",
      "{'loss': 0.5964, 'grad_norm': 9.434494018554688, 'learning_rate': 7.971483736818655e-06, 'epoch': 1.28}\n",
      "{'loss': 0.5928, 'grad_norm': 15.820089340209961, 'learning_rate': 7.674439328679638e-06, 'epoch': 1.31}\n",
      "{'loss': 0.5844, 'grad_norm': 16.37093162536621, 'learning_rate': 7.377394920540621e-06, 'epoch': 1.34}\n",
      "{'loss': 0.5834, 'grad_norm': 15.626032829284668, 'learning_rate': 7.080350512401605e-06, 'epoch': 1.36}\n",
      "{'loss': 0.5791, 'grad_norm': 21.38297462463379, 'learning_rate': 6.783306104262588e-06, 'epoch': 1.39}\n",
      "{'loss': 0.5573, 'grad_norm': 6.909436225891113, 'learning_rate': 6.486855784939849e-06, 'epoch': 1.42}\n",
      "{'loss': 0.5916, 'grad_norm': 8.904561042785645, 'learning_rate': 6.189811376800833e-06, 'epoch': 1.44}\n",
      "{'loss': 0.5768, 'grad_norm': 18.61690330505371, 'learning_rate': 5.892766968661815e-06, 'epoch': 1.47}\n",
      "{'loss': 0.5673, 'grad_norm': 13.497700691223145, 'learning_rate': 5.595722560522798e-06, 'epoch': 1.5}\n",
      "{'loss': 0.5729, 'grad_norm': 12.624029159545898, 'learning_rate': 5.29927224120006e-06, 'epoch': 1.52}\n",
      "{'loss': 0.5762, 'grad_norm': 6.612317085266113, 'learning_rate': 5.002227833061043e-06, 'epoch': 1.55}\n",
      "{'loss': 0.592, 'grad_norm': 8.38641357421875, 'learning_rate': 4.705183424922026e-06, 'epoch': 1.58}\n",
      "{'loss': 0.5747, 'grad_norm': 18.14383316040039, 'learning_rate': 4.408139016783009e-06, 'epoch': 1.6}\n",
      "{'loss': 0.5755, 'grad_norm': 11.105748176574707, 'learning_rate': 4.111094608643993e-06, 'epoch': 1.63}\n",
      "{'loss': 0.5848, 'grad_norm': 29.006362915039062, 'learning_rate': 3.8140502005049754e-06, 'epoch': 1.66}\n",
      "{'loss': 0.588, 'grad_norm': 35.430049896240234, 'learning_rate': 3.517005792365959e-06, 'epoch': 1.68}\n",
      "{'loss': 0.5669, 'grad_norm': 15.163346290588379, 'learning_rate': 3.2205554730432206e-06, 'epoch': 1.71}\n",
      "{'loss': 0.5686, 'grad_norm': 5.505143165588379, 'learning_rate': 2.923511064904204e-06, 'epoch': 1.74}\n",
      "{'loss': 0.5649, 'grad_norm': 20.37586784362793, 'learning_rate': 2.6270607455814644e-06, 'epoch': 1.76}\n",
      "{'loss': 0.55, 'grad_norm': 11.17922306060791, 'learning_rate': 2.3300163374424477e-06, 'epoch': 1.79}\n",
      "{'loss': 0.5621, 'grad_norm': 26.195343017578125, 'learning_rate': 2.0329719293034313e-06, 'epoch': 1.82}\n",
      "{'loss': 0.574, 'grad_norm': 7.834156513214111, 'learning_rate': 1.7359275211644141e-06, 'epoch': 1.84}\n",
      "{'loss': 0.5712, 'grad_norm': 19.22990608215332, 'learning_rate': 1.4388831130253974e-06, 'epoch': 1.87}\n",
      "{'loss': 0.567, 'grad_norm': 32.381656646728516, 'learning_rate': 1.1418387048863806e-06, 'epoch': 1.9}\n",
      "{'loss': 0.5732, 'grad_norm': 21.53437614440918, 'learning_rate': 8.447942967473637e-07, 'epoch': 1.92}\n",
      "{'loss': 0.5692, 'grad_norm': 16.6649227142334, 'learning_rate': 5.47749888608347e-07, 'epoch': 1.95}\n",
      "{'loss': 0.5888, 'grad_norm': 5.159542560577393, 'learning_rate': 2.507054804693302e-07, 'epoch': 1.98}\n",
      "{'train_runtime': 6237.3547, 'train_samples_per_second': 47.976, 'train_steps_per_second': 5.997, 'train_loss': 0.5876096815105072, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=37406, training_loss=0.5876096815105072, metrics={'train_runtime': 6237.3547, 'train_samples_per_second': 47.976, 'train_steps_per_second': 5.997, 'total_flos': 1.9683749115619344e+16, 'train_loss': 0.5876096815105072, 'epoch': 2.0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=2,                    #Make this 5 for optimal training\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=32,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=500,\n",
    "    learning_rate=2e-5,\n",
    "    warmup_ratio=0.1,\n",
    "    dataloader_num_workers=4,\n",
    "    fp16 = True,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=1,\n",
    ")\n",
    "\n",
    "\n",
    "train_test_split = hf_dataset.train_test_split(test_size=0.3, seed=42)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "test_dataset = train_test_split[\"test\"]\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c652a1209ae4a99afbcdd592d355720",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2004 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6706209778785706, 'eval_runtime': 88.9678, 'eval_samples_per_second': 720.767, 'eval_steps_per_second': 22.525, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('fine_tuned_muril_3\\\\tokenizer_config.json',\n",
       " 'fine_tuned_muril_3\\\\special_tokens_map.json',\n",
       " 'fine_tuned_muril_3\\\\vocab.txt',\n",
       " 'fine_tuned_muril_3\\\\added_tokens.json',\n",
       " 'fine_tuned_muril_3\\\\tokenizer.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = trainer.evaluate()\n",
    "print(results)\n",
    "\n",
    "model.save_pretrained(\"fine_tuned_muril_3\")\n",
    "tokenizer.save_pretrained(\"fine_tuned_muril_3\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doodlegen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
